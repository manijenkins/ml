{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "ML_GROUP038.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manijenkins/ml/blob/develop/ML_GROUP038.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6Ls_Il87XUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbsxtb6X7XUK",
        "colab_type": "code",
        "outputId": "b7628fe3-1a33-43c8-e226-8da2fe1a7812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense \n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier \n",
        "import np_utils \n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.model_selection import KFold \n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MD0dzCv7XUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "import math\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdeykUMy7XUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3-_xiWE7XUV",
        "colab_type": "code",
        "outputId": "ae7ba0b4-950e-487a-9041-3586e009c50f",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "from google.colab import files\n",
        "file = files.upload()\n",
        "\n",
        "data=pd.read_csv(\"ionosphere_data.csv\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-36427878-f9f0-41eb-816e-b52420d6e3f6\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-36427878-f9f0-41eb-816e-b52420d6e3f6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDbgRutj7XUY",
        "colab_type": "code",
        "outputId": "153e63d6-0cc8-4b54-c424-3b647bb733dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(350, 35)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFB_e7Dg7XUd",
        "colab_type": "code",
        "outputId": "cb04150a-2bab-4a7c-fdab-1711c1334e64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "data.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>0</th>\n",
              "      <th>0.99539</th>\n",
              "      <th>-0.05889</th>\n",
              "      <th>0.85243</th>\n",
              "      <th>0.02306</th>\n",
              "      <th>0.83398</th>\n",
              "      <th>-0.37708</th>\n",
              "      <th>1.1</th>\n",
              "      <th>0.03760</th>\n",
              "      <th>0.85243.1</th>\n",
              "      <th>-0.17755</th>\n",
              "      <th>0.59755</th>\n",
              "      <th>-0.44945</th>\n",
              "      <th>0.60536</th>\n",
              "      <th>-0.38223</th>\n",
              "      <th>0.84356</th>\n",
              "      <th>-0.38542</th>\n",
              "      <th>0.58212</th>\n",
              "      <th>-0.32192</th>\n",
              "      <th>0.56971</th>\n",
              "      <th>-0.29674</th>\n",
              "      <th>0.36946</th>\n",
              "      <th>-0.47357</th>\n",
              "      <th>0.56811</th>\n",
              "      <th>-0.51171</th>\n",
              "      <th>0.41078</th>\n",
              "      <th>-0.46168</th>\n",
              "      <th>0.21266</th>\n",
              "      <th>-0.34090</th>\n",
              "      <th>0.42267</th>\n",
              "      <th>-0.54487</th>\n",
              "      <th>0.18641</th>\n",
              "      <th>-0.45300</th>\n",
              "      <th>g</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.18829</td>\n",
              "      <td>0.93035</td>\n",
              "      <td>-0.36156</td>\n",
              "      <td>-0.10868</td>\n",
              "      <td>-0.93597</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.04549</td>\n",
              "      <td>0.50874</td>\n",
              "      <td>-0.67743</td>\n",
              "      <td>0.34432</td>\n",
              "      <td>-0.69707</td>\n",
              "      <td>-0.51685</td>\n",
              "      <td>-0.97515</td>\n",
              "      <td>0.05499</td>\n",
              "      <td>-0.62237</td>\n",
              "      <td>0.33109</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>-0.13151</td>\n",
              "      <td>-0.45300</td>\n",
              "      <td>-0.18056</td>\n",
              "      <td>-0.35734</td>\n",
              "      <td>-0.20332</td>\n",
              "      <td>-0.26569</td>\n",
              "      <td>-0.20468</td>\n",
              "      <td>-0.18401</td>\n",
              "      <td>-0.19040</td>\n",
              "      <td>-0.11593</td>\n",
              "      <td>-0.16626</td>\n",
              "      <td>-0.06288</td>\n",
              "      <td>-0.13738</td>\n",
              "      <td>-0.02447</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.03365</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00485</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.12062</td>\n",
              "      <td>0.88965</td>\n",
              "      <td>0.01198</td>\n",
              "      <td>0.73082</td>\n",
              "      <td>0.05346</td>\n",
              "      <td>0.85443</td>\n",
              "      <td>0.00827</td>\n",
              "      <td>0.54591</td>\n",
              "      <td>0.00299</td>\n",
              "      <td>0.83775</td>\n",
              "      <td>-0.13644</td>\n",
              "      <td>0.75535</td>\n",
              "      <td>-0.08540</td>\n",
              "      <td>0.70887</td>\n",
              "      <td>-0.27502</td>\n",
              "      <td>0.43385</td>\n",
              "      <td>-0.12062</td>\n",
              "      <td>0.57528</td>\n",
              "      <td>-0.40220</td>\n",
              "      <td>0.58984</td>\n",
              "      <td>-0.22145</td>\n",
              "      <td>0.43100</td>\n",
              "      <td>-0.17365</td>\n",
              "      <td>0.60436</td>\n",
              "      <td>-0.24180</td>\n",
              "      <td>0.56045</td>\n",
              "      <td>-0.38238</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.45161</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.71216</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>0.14516</td>\n",
              "      <td>0.54094</td>\n",
              "      <td>-0.39330</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>-0.54467</td>\n",
              "      <td>-0.69975</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.90695</td>\n",
              "      <td>0.51613</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.20099</td>\n",
              "      <td>0.25682</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.32382</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.02401</td>\n",
              "      <td>0.94140</td>\n",
              "      <td>0.06531</td>\n",
              "      <td>0.92106</td>\n",
              "      <td>-0.23255</td>\n",
              "      <td>0.77152</td>\n",
              "      <td>-0.16399</td>\n",
              "      <td>0.52798</td>\n",
              "      <td>-0.20275</td>\n",
              "      <td>0.56409</td>\n",
              "      <td>-0.00712</td>\n",
              "      <td>0.34395</td>\n",
              "      <td>-0.27457</td>\n",
              "      <td>0.52940</td>\n",
              "      <td>-0.21780</td>\n",
              "      <td>0.45107</td>\n",
              "      <td>-0.17813</td>\n",
              "      <td>0.05982</td>\n",
              "      <td>-0.35575</td>\n",
              "      <td>0.02309</td>\n",
              "      <td>-0.52879</td>\n",
              "      <td>0.03286</td>\n",
              "      <td>-0.65158</td>\n",
              "      <td>0.13290</td>\n",
              "      <td>-0.53206</td>\n",
              "      <td>0.02431</td>\n",
              "      <td>-0.62197</td>\n",
              "      <td>-0.05707</td>\n",
              "      <td>-0.59573</td>\n",
              "      <td>-0.04608</td>\n",
              "      <td>-0.65697</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.02337</td>\n",
              "      <td>-0.00592</td>\n",
              "      <td>-0.09924</td>\n",
              "      <td>-0.11949</td>\n",
              "      <td>-0.00763</td>\n",
              "      <td>-0.11824</td>\n",
              "      <td>0.14706</td>\n",
              "      <td>0.06637</td>\n",
              "      <td>0.03786</td>\n",
              "      <td>-0.06302</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>-0.04572</td>\n",
              "      <td>-0.15540</td>\n",
              "      <td>-0.00343</td>\n",
              "      <td>-0.10196</td>\n",
              "      <td>-0.11575</td>\n",
              "      <td>-0.05414</td>\n",
              "      <td>0.01838</td>\n",
              "      <td>0.03669</td>\n",
              "      <td>0.01519</td>\n",
              "      <td>0.00888</td>\n",
              "      <td>0.03513</td>\n",
              "      <td>-0.01535</td>\n",
              "      <td>-0.03240</td>\n",
              "      <td>0.09223</td>\n",
              "      <td>-0.07859</td>\n",
              "      <td>0.00732</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>-0.00039</td>\n",
              "      <td>0.12011</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.97588</td>\n",
              "      <td>-0.10602</td>\n",
              "      <td>0.94601</td>\n",
              "      <td>-0.20800</td>\n",
              "      <td>0.92806</td>\n",
              "      <td>-0.28350</td>\n",
              "      <td>0.85996</td>\n",
              "      <td>-0.27342</td>\n",
              "      <td>0.79766</td>\n",
              "      <td>-0.47929</td>\n",
              "      <td>0.78225</td>\n",
              "      <td>-0.50764</td>\n",
              "      <td>0.74628</td>\n",
              "      <td>-0.61436</td>\n",
              "      <td>0.57945</td>\n",
              "      <td>-0.68086</td>\n",
              "      <td>0.37852</td>\n",
              "      <td>-0.73641</td>\n",
              "      <td>0.36324</td>\n",
              "      <td>-0.76562</td>\n",
              "      <td>0.31898</td>\n",
              "      <td>-0.79753</td>\n",
              "      <td>0.22792</td>\n",
              "      <td>-0.81634</td>\n",
              "      <td>0.13659</td>\n",
              "      <td>-0.82510</td>\n",
              "      <td>0.04606</td>\n",
              "      <td>-0.82395</td>\n",
              "      <td>-0.04262</td>\n",
              "      <td>-0.81318</td>\n",
              "      <td>-0.13832</td>\n",
              "      <td>-0.80975</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.96355</td>\n",
              "      <td>-0.07198</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.14333</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.21313</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.36174</td>\n",
              "      <td>0.92570</td>\n",
              "      <td>-0.43569</td>\n",
              "      <td>0.94510</td>\n",
              "      <td>-0.40668</td>\n",
              "      <td>0.90392</td>\n",
              "      <td>-0.46381</td>\n",
              "      <td>0.98305</td>\n",
              "      <td>-0.35257</td>\n",
              "      <td>0.84537</td>\n",
              "      <td>-0.66020</td>\n",
              "      <td>0.75346</td>\n",
              "      <td>-0.60589</td>\n",
              "      <td>0.69637</td>\n",
              "      <td>-0.64225</td>\n",
              "      <td>0.85106</td>\n",
              "      <td>-0.65440</td>\n",
              "      <td>0.57577</td>\n",
              "      <td>-0.69712</td>\n",
              "      <td>0.25435</td>\n",
              "      <td>-0.63919</td>\n",
              "      <td>0.45114</td>\n",
              "      <td>-0.72779</td>\n",
              "      <td>0.38895</td>\n",
              "      <td>-0.73420</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.01864</td>\n",
              "      <td>-0.08459</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.11470</td>\n",
              "      <td>-0.26810</td>\n",
              "      <td>-0.45663</td>\n",
              "      <td>-0.38172</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>-0.33656</td>\n",
              "      <td>0.38602</td>\n",
              "      <td>-0.37133</td>\n",
              "      <td>0.15018</td>\n",
              "      <td>0.63728</td>\n",
              "      <td>0.22115</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>-0.14803</td>\n",
              "      <td>-0.01326</td>\n",
              "      <td>0.20645</td>\n",
              "      <td>-0.02294</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.16595</td>\n",
              "      <td>0.24086</td>\n",
              "      <td>-0.08208</td>\n",
              "      <td>0.38065</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.06655</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.18388</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.27320</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.43107</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.41349</td>\n",
              "      <td>0.96232</td>\n",
              "      <td>-0.51874</td>\n",
              "      <td>0.90711</td>\n",
              "      <td>-0.59017</td>\n",
              "      <td>0.89230</td>\n",
              "      <td>-0.66474</td>\n",
              "      <td>0.69876</td>\n",
              "      <td>-0.70997</td>\n",
              "      <td>0.70645</td>\n",
              "      <td>-0.76320</td>\n",
              "      <td>0.63081</td>\n",
              "      <td>-0.80544</td>\n",
              "      <td>0.55867</td>\n",
              "      <td>-0.89128</td>\n",
              "      <td>0.47211</td>\n",
              "      <td>-0.86500</td>\n",
              "      <td>0.40303</td>\n",
              "      <td>-0.83675</td>\n",
              "      <td>0.30996</td>\n",
              "      <td>-0.89093</td>\n",
              "      <td>0.22995</td>\n",
              "      <td>-0.89158</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   1  0  0.99539  -0.05889  0.85243  ...  0.42267  -0.54487  0.18641  -0.45300  g\n",
              "0  1  0  1.00000  -0.18829  0.93035  ... -0.16626  -0.06288 -0.13738  -0.02447  b\n",
              "1  1  0  1.00000  -0.03365  1.00000  ...  0.60436  -0.24180  0.56045  -0.38238  g\n",
              "2  1  0  1.00000  -0.45161  1.00000  ...  0.25682   1.00000 -0.32382   1.00000  b\n",
              "3  1  0  1.00000  -0.02401  0.94140  ... -0.05707  -0.59573 -0.04608  -0.65697  g\n",
              "4  1  0  0.02337  -0.00592 -0.09924  ...  0.00000   0.00000 -0.00039   0.12011  b\n",
              "5  1  0  0.97588  -0.10602  0.94601  ... -0.04262  -0.81318 -0.13832  -0.80975  g\n",
              "6  0  0  0.00000   0.00000  0.00000  ...  1.00000   1.00000  0.00000   0.00000  b\n",
              "7  1  0  0.96355  -0.07198  1.00000  ...  0.45114  -0.72779  0.38895  -0.73420  g\n",
              "8  1  0 -0.01864  -0.08459  0.00000  ...  0.16595   0.24086 -0.08208   0.38065  b\n",
              "9  1  0  1.00000   0.06655  1.00000  ...  0.30996  -0.89093  0.22995  -0.89158  g\n",
              "\n",
              "[10 rows x 35 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xks6_jJg7XUg",
        "colab_type": "code",
        "outputId": "f7738c6e-e61d-4e5c-b47b-98a39b533b26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "X=data.iloc[:,:-1].values\n",
        "X"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.     ,  0.     ,  1.     , ..., -0.06288, -0.13738, -0.02447],\n",
              "       [ 1.     ,  0.     ,  1.     , ..., -0.2418 ,  0.56045, -0.38238],\n",
              "       [ 1.     ,  0.     ,  1.     , ...,  1.     , -0.32382,  1.     ],\n",
              "       ...,\n",
              "       [ 1.     ,  0.     ,  0.94701, ...,  0.00442,  0.92697, -0.00577],\n",
              "       [ 1.     ,  0.     ,  0.90608, ..., -0.03757,  0.87403, -0.16243],\n",
              "       [ 1.     ,  0.     ,  0.8471 , ..., -0.06678,  0.85764, -0.06151]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSs1bkyc7XUj",
        "colab_type": "code",
        "outputId": "00f18294-9102-4572-8671-971c54d2a9b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "y=data.iloc[:,-1].values\n",
        "y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b',\n",
              "       'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g',\n",
              "       'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b',\n",
              "       'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g',\n",
              "       'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b',\n",
              "       'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g',\n",
              "       'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b',\n",
              "       'g', 'b', 'g', 'b', 'g', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b',\n",
              "       'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g',\n",
              "       'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b',\n",
              "       'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g',\n",
              "       'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b',\n",
              "       'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g',\n",
              "       'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b',\n",
              "       'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g',\n",
              "       'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b',\n",
              "       'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g',\n",
              "       'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b',\n",
              "       'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g', 'b', 'g',\n",
              "       'b', 'g', 'b', 'g', 'b', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g',\n",
              "       'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g',\n",
              "       'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g',\n",
              "       'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g',\n",
              "       'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g',\n",
              "       'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g',\n",
              "       'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g',\n",
              "       'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMEwE4IP7XUm",
        "colab_type": "code",
        "outputId": "c83ebc66-fb82-447b-8e37-a4033a31aa49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(350, 34)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_eme1r17XUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed=7\n",
        "np.random.seed(seed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW1HG3i87XUt",
        "colab_type": "code",
        "outputId": "d0076755-88b1-4b21-b839-4d8d6981f5f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "encoder = LabelEncoder() \n",
        "encoder.fit(y) \n",
        "Y= encoder.transform(y)\n",
        "Y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AepxHl2W7XUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 50\n",
        "def baseline_model(): \n",
        "    # create model \n",
        "    model = Sequential() \n",
        "    model.add(Dense(34, input_dim=34, activation= \"relu\" )) \n",
        "    model.add(Dense(16, activation= \"relu\" ))\n",
        "    model.add(Dense(1, activation= \"sigmoid\" )) \n",
        "    # Compile model\n",
        "     \n",
        "    learning_rate = 0.1 \n",
        "    decay_rate = learning_rate / epochs \n",
        "    momentum = 0.8 \n",
        "    sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
        "    model.compile(loss= \"binary_crossentropy\" , optimizer=sgd, metrics=[ \"accuracy\" ]) \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un74KaFN7XUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=epochs, batch_size=1, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al3OylHe7XU2",
        "colab_type": "code",
        "outputId": "1de88275-57e1-4c1e-ce2f-cbaf4b06aa71",
        "colab": {}
      },
      "source": [
        "# evaluate using 10-fold cross validation\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7410317599773407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-fYDm-p7XU7",
        "colab_type": "code",
        "outputId": "5e549b40-790d-4a21-9327-cfc4c98a54ad",
        "colab": {}
      },
      "source": [
        "estimators = [] \n",
        "estimators.append(( \"standardize\" , StandardScaler())) \n",
        "estimators.append(( \"mlp\" , KerasClassifier(build_fn=baseline_model, nb_epoch=epochs, batch_size=28, verbose=2))) \n",
        "pipeline = Pipeline(estimators) \n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed) \n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold) \n",
        "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 315 samples\n",
            "315/315 - 1s - loss: 0.4838 - accuracy: 0.7968\n",
            "36/1 - 0s - loss: 0.2020 - accuracy: 0.9444\n",
            "Train on 316 samples\n",
            "316/316 - 0s - loss: 0.5314 - accuracy: 0.7880\n",
            "35/1 - 0s - loss: 0.4889 - accuracy: 0.8286\n",
            "Train on 316 samples\n",
            "316/316 - 1s - loss: 0.4826 - accuracy: 0.7816\n",
            "35/1 - 0s - loss: 0.2344 - accuracy: 0.9429\n",
            "Train on 316 samples\n",
            "316/316 - 0s - loss: 0.6075 - accuracy: 0.7152\n",
            "35/1 - 0s - loss: 0.3844 - accuracy: 0.8000\n",
            "Train on 316 samples\n",
            "316/316 - 0s - loss: 0.5267 - accuracy: 0.7184\n",
            "35/1 - 0s - loss: 0.2175 - accuracy: 0.9143\n",
            "Train on 316 samples\n",
            "316/316 - 1s - loss: 0.5216 - accuracy: 0.7373\n",
            "35/1 - 0s - loss: 0.2426 - accuracy: 0.8857\n",
            "Train on 316 samples\n",
            "316/316 - 0s - loss: 0.4439 - accuracy: 0.8133\n",
            "35/1 - 0s - loss: 0.3060 - accuracy: 0.8571\n",
            "Train on 316 samples\n",
            "316/316 - 1s - loss: 0.5845 - accuracy: 0.7215\n",
            "35/1 - 0s - loss: 0.3814 - accuracy: 0.8857\n",
            "Train on 316 samples\n",
            "316/316 - 0s - loss: 0.5238 - accuracy: 0.7911\n",
            "35/1 - 0s - loss: 0.2900 - accuracy: 0.8286\n",
            "Train on 316 samples\n",
            "316/316 - 1s - loss: 0.5231 - accuracy: 0.7405\n",
            "35/1 - 0s - loss: 0.2793 - accuracy: 0.9143\n",
            "Larger: 88.02% (4.77%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJQmJLXI7XU_",
        "colab_type": "code",
        "outputId": "2bee61fe-8aec-4e47-acb6-0f262aa25bdd",
        "colab": {}
      },
      "source": [
        "model = Sequential() \n",
        "model.add(Dense(34, input_dim=34, activation= \"relu\" )) \n",
        "model.add(Dense(16, activation= \"relu\" ))\n",
        "model.add(Dense(1, activation= \"sigmoid\" )) \n",
        "# Compile model     \n",
        "learning_rate = 0.1 \n",
        "decay_rate = learning_rate / epochs \n",
        "momentum = 0.8 \n",
        "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
        "model.compile(loss= \"binary_crossentropy\" , optimizer=sgd, metrics=[ \"accuracy\" ])\n",
        "model.fit(X, Y, validation_split=0.33, nb_epoch=epochs, batch_size=28, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 235 samples, validate on 116 samples\n",
            "Epoch 1/50\n",
            "235/235 - 3s - loss: 0.6253 - accuracy: 0.6468 - val_loss: 0.6501 - val_accuracy: 0.6121\n",
            "Epoch 2/50\n",
            "235/235 - 0s - loss: 0.4516 - accuracy: 0.8298 - val_loss: 0.4325 - val_accuracy: 0.8793\n",
            "Epoch 3/50\n",
            "235/235 - 0s - loss: 0.3222 - accuracy: 0.8766 - val_loss: 0.3541 - val_accuracy: 0.9138\n",
            "Epoch 4/50\n",
            "235/235 - 0s - loss: 0.2428 - accuracy: 0.9319 - val_loss: 0.0974 - val_accuracy: 0.9741\n",
            "Epoch 5/50\n",
            "235/235 - 0s - loss: 0.2062 - accuracy: 0.9319 - val_loss: 0.0877 - val_accuracy: 0.9828\n",
            "Epoch 6/50\n",
            "235/235 - 0s - loss: 0.1366 - accuracy: 0.9660 - val_loss: 0.1045 - val_accuracy: 0.9828\n",
            "Epoch 7/50\n",
            "235/235 - 0s - loss: 0.1168 - accuracy: 0.9617 - val_loss: 0.0597 - val_accuracy: 0.9828\n",
            "Epoch 8/50\n",
            "235/235 - 0s - loss: 0.1149 - accuracy: 0.9489 - val_loss: 0.0497 - val_accuracy: 0.9828\n",
            "Epoch 9/50\n",
            "235/235 - 0s - loss: 0.1293 - accuracy: 0.9660 - val_loss: 0.0595 - val_accuracy: 0.9828\n",
            "Epoch 10/50\n",
            "235/235 - 0s - loss: 0.1105 - accuracy: 0.9574 - val_loss: 0.0458 - val_accuracy: 0.9828\n",
            "Epoch 11/50\n",
            "235/235 - 0s - loss: 0.0892 - accuracy: 0.9702 - val_loss: 0.0430 - val_accuracy: 0.9914\n",
            "Epoch 12/50\n",
            "235/235 - 0s - loss: 0.0974 - accuracy: 0.9745 - val_loss: 0.0539 - val_accuracy: 0.9828\n",
            "Epoch 13/50\n",
            "235/235 - 0s - loss: 0.0629 - accuracy: 0.9830 - val_loss: 0.0659 - val_accuracy: 0.9914\n",
            "Epoch 14/50\n",
            "235/235 - 0s - loss: 0.0594 - accuracy: 0.9830 - val_loss: 0.0600 - val_accuracy: 0.9914\n",
            "Epoch 15/50\n",
            "235/235 - 0s - loss: 0.0565 - accuracy: 0.9872 - val_loss: 0.0977 - val_accuracy: 0.9828\n",
            "Epoch 16/50\n",
            "235/235 - 0s - loss: 0.0554 - accuracy: 0.9830 - val_loss: 0.0519 - val_accuracy: 0.9914\n",
            "Epoch 17/50\n",
            "235/235 - 0s - loss: 0.0531 - accuracy: 0.9830 - val_loss: 0.0684 - val_accuracy: 0.9914\n",
            "Epoch 18/50\n",
            "235/235 - 0s - loss: 0.0452 - accuracy: 0.9872 - val_loss: 0.0718 - val_accuracy: 0.9914\n",
            "Epoch 19/50\n",
            "235/235 - 0s - loss: 0.0574 - accuracy: 0.9872 - val_loss: 0.0682 - val_accuracy: 0.9828\n",
            "Epoch 20/50\n",
            "235/235 - 0s - loss: 0.0425 - accuracy: 0.9872 - val_loss: 0.0457 - val_accuracy: 0.9914\n",
            "Epoch 21/50\n",
            "235/235 - 0s - loss: 0.0498 - accuracy: 0.9787 - val_loss: 0.0438 - val_accuracy: 0.9914\n",
            "Epoch 22/50\n",
            "235/235 - 0s - loss: 0.0541 - accuracy: 0.9915 - val_loss: 0.0483 - val_accuracy: 0.9914\n",
            "Epoch 23/50\n",
            "235/235 - 0s - loss: 0.0448 - accuracy: 0.9915 - val_loss: 0.0483 - val_accuracy: 0.9914\n",
            "Epoch 24/50\n",
            "235/235 - 0s - loss: 0.0347 - accuracy: 0.9915 - val_loss: 0.0796 - val_accuracy: 0.9655\n",
            "Epoch 25/50\n",
            "235/235 - 0s - loss: 0.0536 - accuracy: 0.9872 - val_loss: 0.0551 - val_accuracy: 0.9914\n",
            "Epoch 26/50\n",
            "235/235 - 0s - loss: 0.0315 - accuracy: 0.9915 - val_loss: 0.0490 - val_accuracy: 0.9914\n",
            "Epoch 27/50\n",
            "235/235 - 0s - loss: 0.0291 - accuracy: 0.9957 - val_loss: 0.0467 - val_accuracy: 0.9914\n",
            "Epoch 28/50\n",
            "235/235 - 0s - loss: 0.0283 - accuracy: 0.9957 - val_loss: 0.0510 - val_accuracy: 0.9914\n",
            "Epoch 29/50\n",
            "235/235 - 0s - loss: 0.0266 - accuracy: 0.9957 - val_loss: 0.0458 - val_accuracy: 0.9914\n",
            "Epoch 30/50\n",
            "235/235 - 0s - loss: 0.0241 - accuracy: 0.9915 - val_loss: 0.1753 - val_accuracy: 0.9483\n",
            "Epoch 31/50\n",
            "235/235 - 0s - loss: 0.0588 - accuracy: 0.9830 - val_loss: 0.0587 - val_accuracy: 0.9828\n",
            "Epoch 32/50\n",
            "235/235 - 0s - loss: 0.0318 - accuracy: 0.9915 - val_loss: 0.0513 - val_accuracy: 0.9914\n",
            "Epoch 33/50\n",
            "235/235 - 0s - loss: 0.0253 - accuracy: 0.9957 - val_loss: 0.0692 - val_accuracy: 0.9741\n",
            "Epoch 34/50\n",
            "235/235 - 0s - loss: 0.0261 - accuracy: 0.9957 - val_loss: 0.0535 - val_accuracy: 0.9914\n",
            "Epoch 35/50\n",
            "235/235 - 0s - loss: 0.0258 - accuracy: 0.9957 - val_loss: 0.0633 - val_accuracy: 0.9914\n",
            "Epoch 36/50\n",
            "235/235 - 0s - loss: 0.0254 - accuracy: 0.9957 - val_loss: 0.0567 - val_accuracy: 0.9914\n",
            "Epoch 37/50\n",
            "235/235 - 0s - loss: 0.0216 - accuracy: 0.9957 - val_loss: 0.0592 - val_accuracy: 0.9914\n",
            "Epoch 38/50\n",
            "235/235 - 0s - loss: 0.0227 - accuracy: 0.9957 - val_loss: 0.0511 - val_accuracy: 0.9914\n",
            "Epoch 39/50\n",
            "235/235 - 0s - loss: 0.0230 - accuracy: 0.9957 - val_loss: 0.1065 - val_accuracy: 0.9741\n",
            "Epoch 40/50\n",
            "235/235 - 0s - loss: 0.0680 - accuracy: 0.9660 - val_loss: 0.0774 - val_accuracy: 0.9655\n",
            "Epoch 41/50\n",
            "235/235 - 0s - loss: 0.0751 - accuracy: 0.9745 - val_loss: 0.0576 - val_accuracy: 0.9828\n",
            "Epoch 42/50\n",
            "235/235 - 0s - loss: 0.0277 - accuracy: 0.9915 - val_loss: 0.0468 - val_accuracy: 0.9914\n",
            "Epoch 43/50\n",
            "235/235 - 0s - loss: 0.0280 - accuracy: 0.9957 - val_loss: 0.0567 - val_accuracy: 0.9914\n",
            "Epoch 44/50\n",
            "235/235 - 0s - loss: 0.0216 - accuracy: 0.9957 - val_loss: 0.0606 - val_accuracy: 0.9914\n",
            "Epoch 45/50\n",
            "235/235 - 0s - loss: 0.0266 - accuracy: 0.9957 - val_loss: 0.0646 - val_accuracy: 0.9914\n",
            "Epoch 46/50\n",
            "235/235 - 0s - loss: 0.0203 - accuracy: 0.9957 - val_loss: 0.0501 - val_accuracy: 0.9914\n",
            "Epoch 47/50\n",
            "235/235 - 0s - loss: 0.0227 - accuracy: 0.9957 - val_loss: 0.0591 - val_accuracy: 0.9914\n",
            "Epoch 48/50\n",
            "235/235 - 0s - loss: 0.0192 - accuracy: 0.9957 - val_loss: 0.0601 - val_accuracy: 0.9914\n",
            "Epoch 49/50\n",
            "235/235 - 0s - loss: 0.0204 - accuracy: 0.9957 - val_loss: 0.0630 - val_accuracy: 0.9914\n",
            "Epoch 50/50\n",
            "235/235 - 0s - loss: 0.0189 - accuracy: 0.9957 - val_loss: 0.0619 - val_accuracy: 0.9914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x28a8b264dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BC_NLND7XVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def step_decay(epoch): \n",
        "    initial_lrate = 0.1 \n",
        "    drop = 0.5 \n",
        "    epochs_drop = 5.0 \n",
        "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop)) \n",
        "    return lrate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SHmIIJr7XVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential() \n",
        "model.add(Dense(34, input_dim=34, activation= \"relu\" ))\n",
        "model.add(Dense(25, activation= \"relu\" ))\n",
        "model.add(Dense(25, activation= \"relu\" ))\n",
        "model.add(Dense(7, activation= \"relu\" ))\n",
        "model.add(Dense(3, activation= \"relu\" ))\n",
        "model.add(Dense(2, activation= \"relu\" ))\n",
        "\n",
        "model.add(Dense(1, activation= \"sigmoid\" ))\n",
        "sgd = SGD(lr=0.00, momentum=0.95, decay=0.0, nesterov=False) \n",
        "model.compile(loss= \"binary_crossentropy\" , optimizer=sgd, metrics=[ \"accuracy\" ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOCrpxwi7XVK",
        "colab_type": "code",
        "outputId": "71318272-c7bd-4d29-c87b-239e00fc68c5",
        "colab": {}
      },
      "source": [
        "# learning schedule callback \n",
        "lrate = LearningRateScheduler(step_decay) \n",
        "callbacks_list = [lrate] \n",
        "# Fit the model \n",
        "model.fit(X, Y, validation_split=0.33, nb_epoch=50, batch_size=1, callbacks=callbacks_list, verbose=2) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 235 samples, validate on 116 samples\n",
            "Epoch 1/50\n",
            "235/235 - 3s - loss: 0.7264 - accuracy: 0.6255 - val_loss: 0.4802 - val_accuracy: 0.9224\n",
            "Epoch 2/50\n",
            "235/235 - 0s - loss: 0.8969 - accuracy: 0.5064 - val_loss: 0.4008 - val_accuracy: 0.9224\n",
            "Epoch 3/50\n",
            "235/235 - 0s - loss: 0.9198 - accuracy: 0.4979 - val_loss: 0.4193 - val_accuracy: 0.9224\n",
            "Epoch 4/50\n",
            "235/235 - 0s - loss: 0.8380 - accuracy: 0.4766 - val_loss: 0.9236 - val_accuracy: 0.0776\n",
            "Epoch 5/50\n",
            "235/235 - 0s - loss: 0.8345 - accuracy: 0.5149 - val_loss: 1.3345 - val_accuracy: 0.0776\n",
            "Epoch 6/50\n",
            "235/235 - 0s - loss: 0.7528 - accuracy: 0.5149 - val_loss: 0.7844 - val_accuracy: 0.0776\n",
            "Epoch 7/50\n",
            "235/235 - 0s - loss: 0.7501 - accuracy: 0.5191 - val_loss: 0.4593 - val_accuracy: 0.9224\n",
            "Epoch 8/50\n",
            "235/235 - 0s - loss: 0.7587 - accuracy: 0.5106 - val_loss: 0.5049 - val_accuracy: 0.9224\n",
            "Epoch 9/50\n",
            "235/235 - 1s - loss: 0.7770 - accuracy: 0.5277 - val_loss: 0.3617 - val_accuracy: 0.9224\n",
            "Epoch 10/50\n",
            "235/235 - 0s - loss: 0.7170 - accuracy: 0.5277 - val_loss: 0.6931 - val_accuracy: 0.9224\n",
            "Epoch 11/50\n",
            "235/235 - 0s - loss: 0.7142 - accuracy: 0.5149 - val_loss: 0.5565 - val_accuracy: 0.9224\n",
            "Epoch 12/50\n",
            "235/235 - 0s - loss: 0.7199 - accuracy: 0.5064 - val_loss: 0.6218 - val_accuracy: 0.9224\n",
            "Epoch 13/50\n",
            "235/235 - 1s - loss: 0.7313 - accuracy: 0.4723 - val_loss: 0.5558 - val_accuracy: 0.9224\n",
            "Epoch 14/50\n",
            "235/235 - 0s - loss: 0.7192 - accuracy: 0.4766 - val_loss: 0.9721 - val_accuracy: 0.0776\n",
            "Epoch 15/50\n",
            "235/235 - 0s - loss: 0.7060 - accuracy: 0.5234 - val_loss: 0.7657 - val_accuracy: 0.0776\n",
            "Epoch 16/50\n",
            "235/235 - 0s - loss: 0.6977 - accuracy: 0.5106 - val_loss: 0.4967 - val_accuracy: 0.9224\n",
            "Epoch 17/50\n",
            "235/235 - 0s - loss: 0.7045 - accuracy: 0.5574 - val_loss: 1.0273 - val_accuracy: 0.0776\n",
            "Epoch 18/50\n",
            "235/235 - 0s - loss: 0.7190 - accuracy: 0.4511 - val_loss: 0.8016 - val_accuracy: 0.0776\n",
            "Epoch 19/50\n",
            "235/235 - 0s - loss: 0.7216 - accuracy: 0.4851 - val_loss: 0.9273 - val_accuracy: 0.0776\n",
            "Epoch 20/50\n",
            "235/235 - 1s - loss: 0.7025 - accuracy: 0.5191 - val_loss: 0.7419 - val_accuracy: 0.0776\n",
            "Epoch 21/50\n",
            "235/235 - 0s - loss: 0.7044 - accuracy: 0.4809 - val_loss: 0.5923 - val_accuracy: 0.9224\n",
            "Epoch 22/50\n",
            "235/235 - 0s - loss: 0.6986 - accuracy: 0.5234 - val_loss: 0.7736 - val_accuracy: 0.0776\n",
            "Epoch 23/50\n",
            "235/235 - 0s - loss: 0.7055 - accuracy: 0.4426 - val_loss: 0.7457 - val_accuracy: 0.0776\n",
            "Epoch 24/50\n",
            "235/235 - 1s - loss: 0.6963 - accuracy: 0.5106 - val_loss: 0.5588 - val_accuracy: 0.9224\n",
            "Epoch 25/50\n",
            "235/235 - 0s - loss: 0.7043 - accuracy: 0.5021 - val_loss: 0.6775 - val_accuracy: 0.9224\n",
            "Epoch 26/50\n",
            "235/235 - 0s - loss: 0.6981 - accuracy: 0.4468 - val_loss: 0.6741 - val_accuracy: 0.9224\n",
            "Epoch 27/50\n",
            "235/235 - 0s - loss: 0.7002 - accuracy: 0.4894 - val_loss: 0.7177 - val_accuracy: 0.0776\n",
            "Epoch 28/50\n",
            "235/235 - 0s - loss: 0.6979 - accuracy: 0.4638 - val_loss: 0.6487 - val_accuracy: 0.9224\n",
            "Epoch 29/50\n",
            "235/235 - 0s - loss: 0.7017 - accuracy: 0.5064 - val_loss: 0.7818 - val_accuracy: 0.0776\n",
            "Epoch 30/50\n",
            "235/235 - 0s - loss: 0.6970 - accuracy: 0.5106 - val_loss: 0.6884 - val_accuracy: 0.9224\n",
            "Epoch 31/50\n",
            "235/235 - 0s - loss: 0.6963 - accuracy: 0.4766 - val_loss: 0.6533 - val_accuracy: 0.9224\n",
            "Epoch 32/50\n",
            "235/235 - 0s - loss: 0.6964 - accuracy: 0.4851 - val_loss: 0.7115 - val_accuracy: 0.0776\n",
            "Epoch 33/50\n",
            "235/235 - 0s - loss: 0.6955 - accuracy: 0.4936 - val_loss: 0.7092 - val_accuracy: 0.0776\n",
            "Epoch 34/50\n",
            "235/235 - 0s - loss: 0.6959 - accuracy: 0.4809 - val_loss: 0.6877 - val_accuracy: 0.9224\n",
            "Epoch 35/50\n",
            "235/235 - 0s - loss: 0.6947 - accuracy: 0.4553 - val_loss: 0.6881 - val_accuracy: 0.9224\n",
            "Epoch 36/50\n",
            "235/235 - 0s - loss: 0.6949 - accuracy: 0.4468 - val_loss: 0.7046 - val_accuracy: 0.0776\n",
            "Epoch 37/50\n",
            "235/235 - 0s - loss: 0.6953 - accuracy: 0.4723 - val_loss: 0.6990 - val_accuracy: 0.0776\n",
            "Epoch 38/50\n",
            "235/235 - 0s - loss: 0.6946 - accuracy: 0.4511 - val_loss: 0.6893 - val_accuracy: 0.9224\n",
            "Epoch 39/50\n",
            "235/235 - 1s - loss: 0.6956 - accuracy: 0.5021 - val_loss: 0.6821 - val_accuracy: 0.9224\n",
            "Epoch 40/50\n",
            "235/235 - 0s - loss: 0.6940 - accuracy: 0.4340 - val_loss: 0.6935 - val_accuracy: 0.0776\n",
            "Epoch 41/50\n",
            "235/235 - 0s - loss: 0.6947 - accuracy: 0.4553 - val_loss: 0.6968 - val_accuracy: 0.0776\n",
            "Epoch 42/50\n",
            "235/235 - 0s - loss: 0.6937 - accuracy: 0.4596 - val_loss: 0.6913 - val_accuracy: 0.9224\n",
            "Epoch 43/50\n",
            "235/235 - 0s - loss: 0.6937 - accuracy: 0.4468 - val_loss: 0.6855 - val_accuracy: 0.9224\n",
            "Epoch 44/50\n",
            "235/235 - 0s - loss: 0.6940 - accuracy: 0.5021 - val_loss: 0.6923 - val_accuracy: 0.9224\n",
            "Epoch 45/50\n",
            "235/235 - 0s - loss: 0.6934 - accuracy: 0.4255 - val_loss: 0.6950 - val_accuracy: 0.0776\n",
            "Epoch 46/50\n",
            "235/235 - 0s - loss: 0.6935 - accuracy: 0.4766 - val_loss: 0.6933 - val_accuracy: 0.0776\n",
            "Epoch 47/50\n",
            "235/235 - 0s - loss: 0.6934 - accuracy: 0.4340 - val_loss: 0.6931 - val_accuracy: 0.9224\n",
            "Epoch 48/50\n",
            "235/235 - 0s - loss: 0.6939 - accuracy: 0.4894 - val_loss: 0.6913 - val_accuracy: 0.9224\n",
            "Epoch 49/50\n",
            "235/235 - 0s - loss: 0.6936 - accuracy: 0.4553 - val_loss: 0.6948 - val_accuracy: 0.0776\n",
            "Epoch 50/50\n",
            "235/235 - 0s - loss: 0.6933 - accuracy: 0.4723 - val_loss: 0.6929 - val_accuracy: 0.9224\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x28a8b612f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmfa4II97XVN",
        "colab_type": "code",
        "outputId": "6c2fbf59-a8bb-46e1-f7af-96a9515e3dbf",
        "colab": {}
      },
      "source": [
        "Y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    }
  ]
}